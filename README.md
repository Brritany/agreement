# agreement

Inter-rater reliability

In statistics, inter-rater reliability (also called by various similar names, such as inter-rater agreement, inter-rater concordance, inter-observer reliability, inter-coder reliability, and so on) is the degree of agreement among independent observers who rate, code, or assess the same phenomenon.


# Cohen's kappa
Cohenâ€™s Kappa is a metric used to measure the agreement of two raters. Use `sklearn.metrics.cohen_kappa_score`
